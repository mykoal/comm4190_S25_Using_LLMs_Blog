[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael’s Musings",
    "section": "",
    "text": "LLMs Try to Recreate Pac-Man\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncode\n\n\n\nCan LLMs build a beloved game with minimal prompting?\n\n\n\n\n\nFeb 6, 2024\n\n\nMichael\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating a Profile Picture\n\n\n\n\n\n\nDall-E\n\n\n\nPlaying with Dall-e\n\n\n\n\n\nFeb 4, 2024\n\n\nMichael Li\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_post_1/index.html",
    "href": "posts/001_post_1/index.html",
    "title": "Generating a Profile Picture",
    "section": "",
    "text": "While looking through the Blog content, I was drawn to the idea of generating my own profile pictur using Dall-E 3. I immediately looked into generating my profile picture through some Python scripts in a notebook, but unfortunately, a lot of the documentation I was reviewing for image generation ended up being outdated (curse you openai updates&gt;=1.0.0). I thought it’d be way cooler to run a blog with code that I personally wrote, but I’ll be using Dall-E 3 in OpenAI’s native web app with the 4o model.\n\n\nTo begin, I wanted to explore how Dall-E 3 percieved me based on a few personal details, and build from there.\nSo, who am I?:\nI’m an Asian college man in my 21s studying computer science and communications. I have an orange cat and enjoy swimming. I want my profile picture to reflect these parts of my identity.\nThus, I had GPT 4o prompt engineer the text below for an optimized image of who I described myself to be above:\nA portrait of an Asian college man in his 21s who is a CS and communications major, with an orange cat by his side and a hint of water in the background symbolizing his love for swimming. Realistic digital art, warm lighting.\nOutput: \nThis image is a good starting point, but it feels a bit generic. I’d like to inject more personality and a stylized look.\nI now wanted to refine the prompt to specify a more modern, digital illustration style that brings out my personality leading to prompt 2:\nA modern digital illustration of an Asian college man in his 21s, majoring in computer science and communications, accompanied by his vibrant orange cat. The background subtly hints at swimming with abstract water elements. Minimalist design, bold lines, and a clean vector style.\nOutput: \nI appreciate the crisp vector look, and the text is a nice touch. It even made the ears pierced haha. Now, let’s try something with richer details, like a painting style. Thus, we are lead to prompt 3:\nA digital painting of an Asian college man in his 21s who studies computer science and communications, with his friendly orange cat by his side. The scene is set with a serene background featuring a swimming pool and gentle water reflections, evoking a sense of calm and creativity. Rich, warm color palette and artistic brushstrokes.\nOutput: \nReflecting on this, I tried to avoid changing any details pertaining to the generated ‘asian male’. Note that in each generation, the facial features are relatively similar, with the same exact haircut (same hair part too lol)."
  },
  {
    "objectID": "posts/001_post_1/index.html#generating-my-profile-picture",
    "href": "posts/001_post_1/index.html#generating-my-profile-picture",
    "title": "Generating a Profile Picture",
    "section": "",
    "text": "While looking through the Blog content, I was drawn to the idea of generating my own profile pictur using Dall-E 3. I immediately looked into generating my profile picture through some Python scripts in a notebook, but unfortunately, a lot of the documentation I was reviewing for image generation ended up being outdated (curse you openai updates&gt;=1.0.0). I thought it’d be way cooler to run a blog with code that I personally wrote, but I’ll be using Dall-E 3 in OpenAI’s native web app with the 4o model.\n\n\nTo begin, I wanted to explore how Dall-E 3 percieved me based on a few personal details, and build from there.\nSo, who am I?:\nI’m an Asian college man in my 21s studying computer science and communications. I have an orange cat and enjoy swimming. I want my profile picture to reflect these parts of my identity.\nThus, I had GPT 4o prompt engineer the text below for an optimized image of who I described myself to be above:\nA portrait of an Asian college man in his 21s who is a CS and communications major, with an orange cat by his side and a hint of water in the background symbolizing his love for swimming. Realistic digital art, warm lighting.\nOutput: \nThis image is a good starting point, but it feels a bit generic. I’d like to inject more personality and a stylized look.\nI now wanted to refine the prompt to specify a more modern, digital illustration style that brings out my personality leading to prompt 2:\nA modern digital illustration of an Asian college man in his 21s, majoring in computer science and communications, accompanied by his vibrant orange cat. The background subtly hints at swimming with abstract water elements. Minimalist design, bold lines, and a clean vector style.\nOutput: \nI appreciate the crisp vector look, and the text is a nice touch. It even made the ears pierced haha. Now, let’s try something with richer details, like a painting style. Thus, we are lead to prompt 3:\nA digital painting of an Asian college man in his 21s who studies computer science and communications, with his friendly orange cat by his side. The scene is set with a serene background featuring a swimming pool and gentle water reflections, evoking a sense of calm and creativity. Rich, warm color palette and artistic brushstrokes.\nOutput: \nReflecting on this, I tried to avoid changing any details pertaining to the generated ‘asian male’. Note that in each generation, the facial features are relatively similar, with the same exact haircut (same hair part too lol)."
  },
  {
    "objectID": "posts/001_post_1/index.html#deciding-upon-a-profile-picture",
    "href": "posts/001_post_1/index.html#deciding-upon-a-profile-picture",
    "title": "Generating a Profile Picture",
    "section": "Deciding Upon a Profile Picture",
    "text": "Deciding Upon a Profile Picture\nHonestly, none of the images spoke to me very much due to their lack of ability to identify how I personally looked. It’s okay though! I’ve decided on something a little different (and better):"
  },
  {
    "objectID": "posts/002_post/index.html",
    "href": "posts/002_post/index.html",
    "title": "LLMs Try to Recreate Pac-Man",
    "section": "",
    "text": "I’ve always been interested in LLMs ability to generate code, and the best way to go about doing so. Today, I want to look into how well OpenAI’s 4o, o1, and o3-high, are able to recreate Pac-Man. I will be providing all these models with the same prompt:\nWrite a python script that creates a playable version of Pac-Man.\nTo begin, 4o was able to recreate the movement functionality within a maze, but failed to capture score, ghosts, or crisp movement mechanics. Moving was fast in a straight line, and the maze seemed to be visually cut off in the pygame window. Overall, 4o understood the assignment, but failed to implement multiple features of the game. It must be noted, though, that 4o mentioned these features, but was unable to implement them all in one prompt.\n\n\n\nNext, o1 had a better grasp of the features it needed to implement and attempted to do so. As you can see, it did great job of score keeping based on pellets, but didn’t implement ghost movement logic or restart game functionality. The movement was a lot more clean though, allowing for easier direction switching. When reviewing the “though-process” of o1, it seems like it attempted to implement random ghost movement, but failed somewhere along the line, so I’ll at least give it credit for trying haha.\n\n\n\n\nBest performance, by far. Failed to implement score, but I’m sure somewhere in its thought window, it decided to implement PacMan with basic functionality, which it did very well. There is a ghost with randomized movement, pellets, clean player movement, and the ability to restart. I wonder how well I could make it if I continued to prompt deeper and deeper with o3!\n\n\n\n\nYou may notice that the size of each gameboard is different too. This is because each implementation allowed for easy editing. For example, in o3-high, the code was written as:\n# ----------------------------\n# Configuration and Maze Setup\n# ----------------------------\n# Define a simple maze layout.\n# Characters meaning:\n#   '#' – wall\n#   '.' – pellet (food)\n#   ' ' – empty corridor\n#   'G' – ghost starting position\nmaze_layout = [\n    \"####################\",\n    \"#........#.........#\",\n    \"#.####.#.#.####.#..#\",\n    \"#.................G#\",\n    \"#.####.#####.####..#\",\n    \"#..................#\",\n    \"####################\"\n]\nNote that o1 thought for 15 seconds, and o3-high thought for 89 seconds. I’m really excited to see how impressive models can get as they are designed for greater “internal” thinking. However, is this the death of chain of thought prompting? Especially if it’s harder to change a decision a model made 1 minute into thinking in the frame of a 2 minute thought window.\nWatching the evolution of GPT’s programming performance is really nostalgic, especilly since I’ve been using the models to code as they’ve been released. I’m personally super excited for the ‘deep research’ model! Would love to use it if Ann\nI hope you enjoy this style of gifs. I honestly really enjoy exploring the coding capabilities of LLMs, so expect more of these experiments :)"
  },
  {
    "objectID": "posts/002_post/index.html#reproducing-pac-man-with-llms",
    "href": "posts/002_post/index.html#reproducing-pac-man-with-llms",
    "title": "LLMs Try to Recreate Pac-Man",
    "section": "",
    "text": "I’ve always been interested in LLMs ability to generate code, and the best way to go about doing so. Today, I want to look into how well OpenAI’s 4o, o1, and o3-high, are able to recreate Pac-Man. I will be providing all these models with the same prompt:\nWrite a python script that creates a playable version of Pac-Man.\nTo begin, 4o was able to recreate the movement functionality within a maze, but failed to capture score, ghosts, or crisp movement mechanics. Moving was fast in a straight line, and the maze seemed to be visually cut off in the pygame window. Overall, 4o understood the assignment, but failed to implement multiple features of the game. It must be noted, though, that 4o mentioned these features, but was unable to implement them all in one prompt.\n\n\n\nNext, o1 had a better grasp of the features it needed to implement and attempted to do so. As you can see, it did great job of score keeping based on pellets, but didn’t implement ghost movement logic or restart game functionality. The movement was a lot more clean though, allowing for easier direction switching. When reviewing the “though-process” of o1, it seems like it attempted to implement random ghost movement, but failed somewhere along the line, so I’ll at least give it credit for trying haha.\n\n\n\n\nBest performance, by far. Failed to implement score, but I’m sure somewhere in its thought window, it decided to implement PacMan with basic functionality, which it did very well. There is a ghost with randomized movement, pellets, clean player movement, and the ability to restart. I wonder how well I could make it if I continued to prompt deeper and deeper with o3!\n\n\n\n\nYou may notice that the size of each gameboard is different too. This is because each implementation allowed for easy editing. For example, in o3-high, the code was written as:\n# ----------------------------\n# Configuration and Maze Setup\n# ----------------------------\n# Define a simple maze layout.\n# Characters meaning:\n#   '#' – wall\n#   '.' – pellet (food)\n#   ' ' – empty corridor\n#   'G' – ghost starting position\nmaze_layout = [\n    \"####################\",\n    \"#........#.........#\",\n    \"#.####.#.#.####.#..#\",\n    \"#.................G#\",\n    \"#.####.#####.####..#\",\n    \"#..................#\",\n    \"####################\"\n]\nNote that o1 thought for 15 seconds, and o3-high thought for 89 seconds. I’m really excited to see how impressive models can get as they are designed for greater “internal” thinking. However, is this the death of chain of thought prompting? Especially if it’s harder to change a decision a model made 1 minute into thinking in the frame of a 2 minute thought window.\nWatching the evolution of GPT’s programming performance is really nostalgic, especilly since I’ve been using the models to code as they’ve been released. I’m personally super excited for the ‘deep research’ model! Would love to use it if Ann\nI hope you enjoy this style of gifs. I honestly really enjoy exploring the coding capabilities of LLMs, so expect more of these experiments :)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A little place filled with Michael’s musings"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/00#_template/index.html",
    "href": "posts/00#_template/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/00#_template/index.html#a-visualization-problem-for-llms",
    "href": "posts/00#_template/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  }
]