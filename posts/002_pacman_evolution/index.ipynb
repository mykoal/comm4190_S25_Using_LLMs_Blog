{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLMs Try to Recreate Pac-Man\"\n",
    "description: \"Can LLMs build a beloved game with minimal prompting?\"\n",
    "author: \"Michael\"\n",
    "date: \"2/6/2024\"\n",
    "categories:\n",
    "  - Coding\n",
    "  - Python\n",
    "  - PacMan\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e51c59-1fc5-47d7-95bb-2e592bdac5de",
   "metadata": {},
   "source": [
    "## Reproducing Pac-Man with LLMs\n",
    "\n",
    "I've always been interested in LLMs ability to generate code, and the best way to go about doing so. Today, I want to look into how well OpenAI's 4o, o1, and o3-high, are able to recreate Pac-Man. I will be providing all these models with the same prompt:\n",
    "\n",
    "```\n",
    "Write a python script that creates a playable version of Pac-Man.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11bf56-ea20-4e37-b397-52c8e3140757",
   "metadata": {},
   "source": [
    "To begin, 4o was able to recreate the movement functionality within a maze, but failed to capture score, ghosts, or crisp movement mechanics. Moving was fast in a straight line, and the maze seemed to be visually cut off in the pygame window. Overall, 4o understood the assignment, but failed to implement multiple features of the game. It must be noted, though, that 4o mentioned these features, but was unable to implement them all in one prompt.\n",
    "\n",
    "### 4o PacMan\n",
    "\n",
    "![](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExNTExeWVhaHh5dHNlbnVpdWt2Y3ZsOWh4Nzk5a25hc3Z0NXkzcnhlYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/dn8HeHOK7D4sn2WS4y/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26032a-8882-4b29-bf9f-52ac1de66a8a",
   "metadata": {},
   "source": [
    "Next, o1 had a better grasp of the features it needed to implement and attempted to do so. As you can see, it did great job of score keeping based on pellets, but didn't implement ghost movement logic or restart game functionality. The movement was a lot more clean though, allowing for easier direction switching. When reviewing the \"though-process\" of o1, it seems like it attempted to implement random ghost movement, but failed somewhere along the line, so I'll at least give it credit for trying haha.\n",
    "\n",
    "### o1 PacMan\n",
    "\n",
    "![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExNWZteXkwcTU3N3N3b3R4dmIzZnVreTlldjlxc2VuaHdyaDVtdG15diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/mQI5SaslMV1Qtg2m74/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e9005-d691-400b-a211-c89034882bc6",
   "metadata": {},
   "source": [
    "Best performance, by far. Failed to implement score, but I'm sure somewhere in its thought window, it decided to implement PacMan with basic functionality, which it did very well. There is a ghost with randomized movement, pellets, clean player movement, and the ability to restart. I wonder how well I could make it if I continued to prompt deeper and deeper with o3! \n",
    "\n",
    "### o3-high\n",
    "\n",
    "![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDh6dGljMzlrcHV4bnlrdTV4em9jYXB3eDM2YzEwcTJyOThtOXdieiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/OVGjYbej4RhPtBB5eb/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2c604-d4e5-4972-a79d-7d12ccb9c4e9",
   "metadata": {},
   "source": [
    "You may notice that the size of each gameboard is different too. This is because each implementation allowed for easy editing. For example, in o3-high, the code was written as:\n",
    "```\n",
    "# ----------------------------\n",
    "# Configuration and Maze Setup\n",
    "# ----------------------------\n",
    "# Define a simple maze layout.\n",
    "# Characters meaning:\n",
    "#   '#' – wall\n",
    "#   '.' – pellet (food)\n",
    "#   ' ' – empty corridor\n",
    "#   'G' – ghost starting position\n",
    "maze_layout = [\n",
    "    \"####################\",\n",
    "    \"#........#.........#\",\n",
    "    \"#.####.#.#.####.#..#\",\n",
    "    \"#.................G#\",\n",
    "    \"#.####.#####.####..#\",\n",
    "    \"#..................#\",\n",
    "    \"####################\"\n",
    "]\n",
    "```\n",
    "\n",
    "Note that o1 thought for 15 seconds, and o3-high thought for **89 seconds**. I'm really excited to see how impressive models can get as they are designed for greater \"internal\" thinking. However, is this the death of chain of thought prompting? Especially if it's harder to change a decision a model made 1 minute into thinking in the frame of a 2 minute thought window.\n",
    "\n",
    "Watching the evolution of GPT's programming performance is really nostalgic, especilly since I've been using the models to code as they've been released. I'm personally super excited for the 'deep research' model! Would love to use it if Annenberg was able to get access. \n",
    "\n",
    "I hope you enjoy this style of gifs. I honestly really enjoy exploring the coding capabilities of LLMs, so expect more of these experiments :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bb4f8-a574-41ca-bc02-92d287cebdc7",
   "metadata": {},
   "source": [
    "### Disregard: image for blog\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Pac_Man.svg/657px-Pac_Man.svg.png)"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
