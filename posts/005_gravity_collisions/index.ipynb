{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLMs Take on Gravity and Collisions\"\n",
    "description: \"Can o3-mini, Sonnet 3.5, and R1 simulate gravity and collisions?\"\n",
    "author: \"Michael\"\n",
    "date: \"2/18/2024\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90cbc99-8bf0-424e-a344-4f9bf452591d",
   "metadata": {},
   "source": [
    "## Do LLMs Understand Physics?\n",
    "\n",
    "I was inspired by a tweet I saw of someone comparing the responses of r1 and o3 for a prompt asking to simulate a ball rolling. The replies were filled with 80% \"wow this is so cool, look at the power of AI, it's the future\" filler, 10% was discussing how different LLMs \"intepret physics\", and 10% was ragging on how \"this isn't representative of how coding benchmarks work for AI\".\n",
    "\n",
    "From these comments, I came to 3 conclusions\n",
    "\n",
    "*  The art of \"measuring\" an AI's performance is in constant flux\n",
    "*  I want to learn more about AI benchmarks\n",
    "*  I must write blogs on similar experiments that I run myself\n",
    "\n",
    "This blog is an extension of the third conclusion.\n",
    "\n",
    "For this experiment, I decided to explore R1, Sonnet 3.5, and o3-mini-high's responses.\n",
    "\n",
    "First, the prompt I ultimately decided on was:\n",
    "\n",
    "> Program a python program that simulates random bouncy shapes dropping into a box. Make sure the shapes are effected by gravity and interact with each other. Do not use an existing package to simulate physics.\n",
    "\n",
    "**Note:** I had to tweak the prompt a bit because all the models opted to use the 'pymunk' package which simulates physics... no cheating!!!\n",
    "\n",
    "### DeepSeek R1\n",
    "DeepSeek's R1 was the worst performer. I'm not sure if it interpreted gravity as all the shapes have their own gravitational force, but the shapes all stick together upon colliding. I also noticed that R1 intepreted 'shapes' as only circles, which was an interesting decision it made. In general, R1 had difficulties implementing physics into the simulation. \n",
    "\n",
    "![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExdnphajVldTFvN2V4N2Vvd25qamprZDI5bG8xc3JvaGdsc292OTYwOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZvozRFkaXvwQ63u8E5/giphy.gif)\n",
    "\n",
    "### Claude 3.5 Sonnet\n",
    "Sonnet 3.5... released more than half a year ago, yet still so impressive at coding tasks. While doing a bit worse than o3 (discussed later), it was relatively impressive! It seemed to have opted for squares and circles, generating at random upon user clicks. As for gravity, you can see that shapes bounce more when dropped from higher heights. However, regarding collisions, Sonnet did poorly, having the shapes stick together rather than remain bouncy.\n",
    "\n",
    "![](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExYzltM200eWxidDVzeTc5Y2I1MzR6d3JrdGh0Y2dyazA0c3FpenZyeSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/P9lHWJmpHYmOanjECq/giphy.gif)\n",
    "\n",
    "### GPT o3-mini-high\n",
    "The highest performer, and the newest model on the block. The model was able to account for fall distance in the bounce and acceleration in the collisions dependent on both shapes initial velocities. The model also made a similar decision to have the shapes be squares and circles. For all its impressive traits, the model still failed to account for a 'real-world' simulation, with some random edge cases occuring, an example being when a square on the floor bounces back up after being collided with from above.\n",
    "\n",
    "![](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3lmdWxheGRmbm82cWhjcHN4MW9vdWcxOTY0djdhYmJ6Y3F1ZndoeCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/UqOTWhxpvtPorxXUqN/giphy.gif)\n",
    "\n",
    "\n",
    "## Future Blog Posts\n",
    "This blog post has inspired me to do more experiments like these to understand the extent to which LLMs understand certain natural phenomena and can simulate them through code. I'd also like to look into how people measure the benchmarks to which these models are graded on, particularly in the realm of coding."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
